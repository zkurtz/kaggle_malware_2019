'''
1. Rank the features by some measure of predictive importance.
2. Fit a new logistic regression to discriminate between training and test data based on the top N features from (1)
3. Use (2) to rank the drift of the (1)-important features.
'''
from feather import read_dataframe as read_feather
import pandas as pd
import pdb
import pickle

from zpylib import datatools
from zpylib.learn.gbm import Lgbm
from zpylib import data_path as dp
from zpylib import model_path as mp

# Identify the most-important features based on models previously
#   trained in `malware/build_models/gbm.py`
models = pickle.load(open(mp('lgb_3feather.pkl'), "rb"))
N = 25
KEEPERS = list(set(
    models[0].importance().iloc[:N].feature.tolist()
    + models[1].importance().iloc[:N].feature.tolist()
    + models[2].importance().iloc[:N].feature.tolist()
))
# The fact there are only 26 (or N+1) features in the union shows high agreement
#   among the 3 lgb models

# Load the data for the keepers both for a sample of train and test
def load_data():
    train_df = read_feather(dp("refactored/train_split_0.feather"),
                            columns=KEEPERS).iloc[:500000]
    test_df = read_feather(dp("refactored/test_sample.feather"),
                            columns=KEEPERS)
    train_df['is_test'] = 0
    test_df['is_test'] = 1
    df = pd.concat([train_df, test_df])
    return datatools.Data(df, target='is_test')

data = load_data()

# Fit a model to predict whether a sample is in train or test. The features that are
#   most important for this regression are [arguably] the features that drifted
#   the most between train and test
lgb = Lgbm(verbose=True)
lgb.train(data)
lgb.importance().feature[:10].tolist()


