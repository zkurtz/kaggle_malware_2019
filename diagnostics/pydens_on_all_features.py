'''
1. Rank the features by some measure of predictive importance.
2. Use pydens to do density estimation on the top N (10?) features
3. Check whether
    (a) the estimated density is a useful predictive feature, or
    (b) the test-set mean density differs substantially from the train-set mean density
'''

from feather import read_dataframe as read_feather
import numpy as np
import pdb

import pydens # Install from github:https://github.com/zkurtz/pydens
import zpylib as zp
from zpylib import data_path as dp

target = 'HasDetections'
models_path = zp.model_path('lgb_3feather.pkl')
top_features = zp.model_loaders.which_top_features(models_path, N=100)
top_features_with_target = top_features + [target]

###############
## Load the data for the top features both for a sample of train and test
train_df = read_feather(
    dp("refactored/train_split_0.feather"),
    columns=top_features_with_target).iloc[:500000]
test_df = read_feather(
    dp("refactored/test_sample.feather"),
    columns=top_features)
train_data = zp.datatools.Data(train_df.drop(target, axis=1))
test_data = zp.datatools.Data(test_df)
cats = [f for f in train_data.X.columns if f in train_data.coltypes.categorical]

###############
## Fit a density model
classifier = pydens.classifiers.lightgbm.Lgbm(categorical_features=cats, verbose=True)
num_dens_params = {
    'loner_min_count': 100,
    'binning_params': {'max_bins': 20}
}
cade = pydens.cade.Cade(
    classifier=classifier,
    initial_density = pydens.models.JointDensity(numeric_params=num_dens_params, verbose=2),
    sim_size=500000,
    verbose=True)
diagnostics = cade.train(df=train_data.X, diagnostics=True)
print('classifier AUC = ' + str(diagnostics['auc']))

# deep dive: cade.initial_density.univariates['Census_TotalPhysicalRAM'].crowd_bins
print('Score the train and test data')
train_df['log_density_est'] = np.log(cade.density(train_data.X))
test_df['log_density_est'] = np.log(cade.density(test_data.X))

###############
print('Assess the importance of the density scores in a predictive model')
lgb = zp.learn.gbm.Lgbm(verbose=True)
train_data_with_dens = zp.datatools.Data(
    train_df,
    extra_predictors=['log_density_est']
)
lgb.train(train_data_with_dens)
print(lgb.importance())
pdb.set_trace()